# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Book RAG is a Go HTTP server that ingests books into a vector database (PostgreSQL + pgvector) and provides RAG (Retrieval-Augmented Generation) capabilities. Books are chunked, embedded using Ollama's embeddinggemma model, and stored for semantic search and LLM-enhanced querying via OpenAI.

## Development Commands

```bash
# Environment setup
mise install                          # Installs Go 1.25.4
docker compose up -d                  # Start PostgreSQL + Adminer
ollama pull embeddinggemma            # Download embedding model

# Running
go run main.go                        # Start server on :3000

# Testing
go test ./...                         # Run all tests
go test ./rag/                        # Test specific package

# Database codegen (after changing db/query.sql)
make generate                         # Expands to: rm -rf data/ && sqlc generate

# Create new migration
migrate create -ext sql -dir db/migrations -seq description

# Evaluation pipeline
go run cmd/gendata/main.go -samples 30 -book-id 19    # Generate eval dataset
go run cmd/evaluate/main.go                            # Run evaluation
```

## Evaluation System

### Overview

The project includes an evaluation pipeline for measuring RAG performance using LLM-as-a-judge:

- **Dataset Generation** (`cmd/gendata`): Creates synthetic QA pairs from book passages
- **Quality Filtering**: Three critique agents score groundedness, relevance, and standalone clarity
- **Answer Judging** (`cmd/evaluate`): LLM compares generated vs reference answers (1-5 scale)
- **Metrics**: Average score, pass rate (≥4 threshold), distribution analysis

### Commands

```bash
# Generate 30 QA pairs from book ID 19 (Romeo & Juliet)
go run cmd/gendata/main.go -samples 30 -book-id 19

# Run evaluation (requires server running)
go run cmd/evaluate/main.go

# Custom paths
go run cmd/evaluate/main.go \
  -dataset testdata/my_dataset.json \
  -output testdata/results/v2.json
```

### Current Baseline

- **Average Score**: 4.33 / 5.0
- **Pass Rate**: 0.867 (86.7% scored ≥4)
- **Dataset**: 15 QA pairs from Romeo & Juliet

### Files

- `eval/` - Core evaluation logic (types, dataset_gen, critique, judge, metrics, runner)
- `cmd/gendata/` - CLI for dataset generation
- `cmd/evaluate/` - CLI for running evaluations
- `testdata/eval_dataset.json` - Generated QA pairs (version controlled)
- `testdata/results/baseline.json` - Evaluation results (version controlled)

### Key Implementation Details

- **Parallel Critiques**: Three critique LLM calls run concurrently for speed
- **Quality Threshold**: QA pairs must score ≥3 on all three dimensions
- **Pass Rate**: Expect ~50% of generated QA pairs to pass filtering
- **LLM Model**: Uses GPT-4o-mini for cost-effectiveness

## Architecture

### Component Structure

```
handler/     HTTP layer (API endpoints, request/response handling)
  ↓
rag/         Business logic (chunking, embeddings, LLM generation)
  ↓
data/        Database layer (GENERATED by sqlc - DO NOT EDIT)
  ↓
db/          Database initialization and migrations
```

### Data Flow

**Ingestion (POST /books)**:
1. `handler/ingest.go` receives file/text
2. `rag/chunking.go` splits text into ~1000 char chunks (paragraph-based)
3. `rag/embedding.go` generates vectors via Ollama (batches of 20)
4. `data/batch.go` inserts book + passages in transaction

**Query (POST /books/{id}/query)**:
1. `handler/query.go` receives query text
2. `rag/embedding.go` embeds the query
3. `data/query.sql.go` performs vector similarity search (pgvector `<=>` operator)
4. Returns passages ranked by cosine similarity

**RAG (POST /books/{id}/rag)**:
1. `handler/generate.go` retrieves top 10 passages via query flow
2. `rag/llm.go` constructs prompt with passages as context
3. OpenAI GPT-5 Mini generates answer
4. Returns LLM response

### Database Layer Pattern

This project uses **sqlc** for type-safe database access:
- Write SQL queries in `db/query.sql`
- Define schema in `db/migrations/*.sql`
- Run `make generate` to create Go code in `data/`
- All files in `data/` are auto-generated - NEVER edit them directly

Migration files run automatically on server startup via `db/init.go`.

## Key Technical Details

### Vector Database Setup

- **Extension**: pgvector (PostgreSQL extension)
- **Schema**: Custom `rag` schema with `book` and `book_passage` tables
- **Vector Column**: `embedding VECTOR` (dimension inferred from data)
- **Similarity Metric**: Cosine distance via `<=>` operator
  ```sql
  -- Returns passages ordered by similarity (1 - cosine_distance)
  SELECT CAST(1 - (embedding <=> $2) AS REAL) AS similarity
  FROM rag.book_passage
  ORDER BY embedding <=> $2
  ```

### Embedding Generation

- **Model**: embeddinggemma (via Ollama)
- **API**: Local Ollama at `http://localhost:11434` (configurable via `OLLAMA_BASE_URL`)
- **Batching**: Processes 20 texts per API call to avoid overwhelming Ollama
- **Location**: `rag/embedding.go`
- **Requirement**: Must run `ollama pull embeddinggemma` before use

### Text Chunking Strategy

- **Target Size**: ~1000 characters per chunk
- **Algorithm**: Paragraph-based accumulation
  - Splits on double newlines (`\n\n`)
  - Combines paragraphs until reaching ~1000 chars
  - Never splits mid-paragraph (preserves semantic coherence)
  - Long paragraphs kept intact even if > 1000 chars
- **Location**: `rag/chunking.go`

### RAG Implementation

- **Provider**: OpenAI GPT-5 Mini
- **Context**: Top 10 passages injected into prompt with relevance scores
- **Prompt Pattern**: "You are an assistant in a book publishing company..." with passages as context
- **Location**: `rag/llm.go`, `handler/generate.go`
- **Requirement**: `OPENAI_API_KEY` environment variable must be set

## Database Patterns

### sqlc Workflow

1. Edit SQL queries in `db/query.sql` using sqlc annotations:
   ```sql
   -- name: QueryBook :many
   SELECT id, passage_text FROM rag.book_passage WHERE book_id = $1;
   ```
2. Run `make generate` to regenerate `data/` directory
3. Use generated methods: `db.Queries.QueryBook(ctx, bookID)`

### Batch Insert Pattern

For inserting many passages efficiently:
```go
batch := db.Queries.CreateBookPassages(ctx, /* params */)
defer batch.Close()
batch.Exec(func(i int, err error) {
    // Handle per-row errors
})
```

### Transactions

Use `WithTx` for transactional operations:
```go
tx, _ := db.Pool.Begin(ctx)
defer tx.Rollback()
qtx := db.Queries.WithTx(tx)
// Use qtx for queries...
tx.Commit()
```

## Important Gotchas

### Generated Code
- **Never edit** files in `data/` - they are regenerated by `make generate`
- After changing `db/query.sql` or migrations, always run `make generate`
- The entire `data/` directory is deleted and recreated each time

### External Dependencies
- **Ollama**: Must be running locally with `embeddinggemma` model pulled
  - Without it: ingestion and query endpoints will fail
- **OpenAI API Key**: Required only for `/rag` endpoint
  - Without it: query endpoints work fine, but RAG generation fails

### Migration Strategy
- Migrations run automatically on every server startup (`db/init.go`)
- Checks for `migrate.ErrNoChange` to handle already-applied migrations
- This pattern is suitable for development but should be revised for production

### Global State
- Database connection stored in package-level globals: `db.Queries`, `db.Pool`, `db.Conn`
- Singleton pattern ensures initialization happens once
- All handlers access database through these globals

### Environment Configuration
- Base config: `mise.toml` (committed)
- Local overrides: `mise.local.toml` (gitignored, contains API keys)
- Required vars: `DATABASE_URL`, `OLLAMA_BASE_URL`, `OPENAI_API_KEY`
